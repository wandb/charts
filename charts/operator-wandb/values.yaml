## NOTICE
#
# Due to the scope and complexity of this chart, all possible values are not
# documented in this file. Extensive documentation is available.
#
# Because properties are regularly added, updated, or relocated, it is _strongly
# suggest_ to not "copy and paste" this YAML. Please provide Helm only those
# properties you need, and allow the defaults to be provided by the version of
# this chart at the time of deployment.

# The global properties are used to configure multiple charts at once.
global:
  # This should be the fqdn of where your users will be accessing the instance.
  host: "http://localhost:8080"
  license: ""

  licenseSecret:
    name: ""
    key: ""

  cloudProvider: ""

  storageClass: ""

  banners:
    {}
    # banner1:
    #   type: warning | error | info
    #   message: "This is a warning message"
    #   heading: "This is a warning"
    #   dismissable: true
    # banner2:
    #   type: warning | error | info
    #   message: "This is a warning message"
    #   heading: "This is a warning"

  common:
    labels: {}
    annotations: {}

  ## Supplemental Pod labels. Will not be used for selectors.
  pod:
    labels: {}
    annotations: {}
  deployment:
    annotations: {}
    labels: {}
  service:
    labels: {}
    annotations: {}

  extraEnvFrom: {}
  extraEnv:
    BUCKET_QUEUE: internal://

  operator:
    namespace: default

  mysql:
    host: ""
    port: 3306
    database: "wandb_local"
    user: "wandb"
    password: ""
    passwordSecret:
      name: ""
      rootPasswordKey: ""
      passwordKey: ""

  slack:
    secret: ""
    clientId: ""

  clickhouse:
    install: false
    host: ""
    port: 8443
    password: "fake"
    database: "weave_trace_db"
    user: "default"

  email:
    smtp:
      host: ""
      port: 587
      user: ""
      password: ""

  auth:
    sessionLengthHours: 720
    oidc:
      clientId: ""
      secret: ""
      authMethod: ""
      issuer: ""

  # Storage bucket that will be used by the application by default but can be overridden by the user in the wandb-console.
  defaultBucket:
    # az, s3, gcs
    provider: "s3"
    name: ""
    path: ""
    region: ""
    kmsKey: ""
    secretKey: ""
    accessKey: ""
    secretName: ""

  # If specified the application will use this bucket for all storage operations, and will not be overridable by the user.
  bucket:
    secretName: ""
    accessKeyName: ""
    secretAccessKeyName: ""

  redis:
    host: ""
    port: 6379
    password: ""
    parameters: {}
    caCert: ""
    # The name of the user supplied secret containing the password for the redis instance.
    secretName: ""
    secretKey: ""

  kafka:
    # The following values are anchored here, and referenced by alias later for
    # the kafka dependency chart.
    user: &kafkaUser "wandb"
    password: &kafkaPassword "wandb"
    brokerHost: ""
    brokerPort: 9092
    runUpdatesShadowTopic: ""
    # This value will only apply upon initial topic creation.
    # If the topic already exists then changing the number of partitions is not possible.
    runUpdatesShadowNumPartitions: 12

# To provide custom CA certificates, you can use either:
# 1. `customCACerts`: a list of certificates provided directly within this Helm chart.
# 2. `configMapName`: the name of a ConfigMap containing CA certificates.
#
# Important:
# - If using a ConfigMap, each key in the ConfigMap must end with `.crt` (e.g., `my-cert.crt`).
# - This naming convention is required for `update-ca-certificates` to parse and add each
#   certificate to the system CA store on Ubuntu-based systems.

# List of custom CA certificates in PEM format.
  customCACerts: []
# Name of a ConfigMap containing additional .crt files for CA certificates.
  caCertsConfigMap: ""

  weave-trace:
    enabled: false

ingress:
  install: true
  create: true
  nameOverride: ""
  defaultBackend: "app"
  annotations: {}
  labels: {}
  tls: []
  additionalHosts: []
  class: ""
  issuer:
    create: false
    provider: ""
    server: https://acme-v02.api.letsencrypt.org/directory
    email: support@wandb.com
  secondary:
    create: false
    install: true
    nameOverride: ""
    defaultBackend: "app"
    annotations: {}
    labels: {}
    tls: []
    additionalHosts: []
    class: ""
    issuer:
      create: false
      provider: ""
      server: https://acme-v02.api.letsencrypt.org/directory
      email: support@wandb.com

parquet:
  install: true
  image:
    repository: wandb/local
    tag: latest

app:
  install: true
  image:
    repository: wandb/local
    tag: latest

nginx:
  install: false

weave:
  install: true
  image:
    repository: wandb/local
    tag: latest

weave-trace:
  install: false
  image:
    repository: wandb/weave-trace
    tag: latest
  datadog:
    enabled: false

console:
  install: true
  image:
    repository: wandb/console
    tag: latest

flat-run-fields-updater:
  install: false
  image:
    repository: wandb/local
    tag: latest

mysql:
  install: false
  persistence:
    size: 20Gi
    storageClass: ""

yace:
  install: false
  regions: ["ap-south-1"]
  pod:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "5000"
      prometheus.io/path: "/metrics"
      prometheus.io/scheme: http

redis:
  install: true
  nameOverride: "redis"
  architecture: standalone
  auth:
    enabled: false
  metrics:
    enabled: false
    service:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9121"
        prometheus.io/path: "/metrics"

prometheus:
  install: true

  instance:
    install: true

  redis-exporter:
    install: true

  mysql-exporter:
    install: true

stackdriver:
  install: false
  pod:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9255"
      prometheus.io/path: "/metrics"
      prometheus.io/scheme: http

otel:
  install: true

  # Consider deploying with a deployment rather than a daemonset to avoid
  # redundant metrics. In setups where both are scraping the same endpoints,
  # this redundancy occurs as both scrape the same points.
  daemonset:
    install: true

  # By default, we use only the daemonset which, along with the Prometheus
  # receiver, gathers most necessary metrics.
  # - To forward Kafka metrics to an external system, you will need to scrape a
  #   promethus endpoint which causes duplicated metrics that get repoted.
  deployment:
    install: false

#   # cannot use install because schema validation will fail.
#   enabled: true

#   resources:
#     limits:
#       cpu: 100m
#       memory: 200M
#   configMap:
#     create: false

#   command:
#     name: otelcol-contrib
#     extraArgs: ["--config=/conf/config.yaml"]
#   extraVolumes:
#     - name: "otel-config"
#       configMap:
#         name: "otel-config"
#         items:
#           - key: config
#             path: config.yaml
#         defaultMode: 420
#   extraVolumeMounts:
#     - name: "otel-config"
#       mountPath: /conf/config.yaml

nameOverride: ""
fullnameOverride: ""

# It is *strongly* recommended to supply passwords yourself for production installs.
kafka:
  install: false
  controller:
    persistence:
      size: 30Gi
  sasl:
    interbroker:
      password: "inter-broker"
    controller:
      password: "controller-pw"
    # The client usernames and passwords are alias references from the global values section at the beginning of this file.
    # Please update the values there to ensure proper propagation to the application
    client:
      users:
        - *kafkaUser
      passwords: *kafkaPassword
  kraft:
    # This field is a UUID. It is *strongly* recommended to supply a new UUID yourself for production installs.
    clusterId: "ffFF1H3AQKGdBnsqAbJKew"
  metrics:
    jmx:
      enabled: true
